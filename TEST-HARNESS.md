# OTel Web SDK Test Harness

The test harness is a set of tools and scripts that help in testing the OTel Web SDK across different platforms. 
It focuses on three main aspects of testing:

1. **Build Tests**: These tests ensure that the SDKs can be built correctly for each platform. 
They also check the size of the built SDKs and ensure they meet the expected criteria. 
The results of these tests are stored in `tests/integration/build-test-results/`.
2. **End-to-End Tests**: These tests are run against the built SDKs to ensure they work as expected in a real-world scenario. 
A local server is started, and the SDK is configured to point to this server. 
The tests then verify that the SDK can send events and that the server receives them correctly.
3. **Performance Tests**: These tests measure the performance of the SDKs in terms of load time, memory usage, and CPU usage. 
The results of these tests are stored in `tests/integration/test-results/`.


## Running the Test Harness

### Build and End-to-End Tests

To run the build and end-to-end tests, run the following commands:

```bash
npm run sdk:test:integration
```

This command will run both the build tests and the end-to-end tests for all platforms. You can also run them separately:

```bash
npm run sdk:test:integration:build
npm run sdk:test:integration:e2e
```

Just make sure that the build tests are run before the end-to-end tests, as the latter depend on the former. If there
are no changes in the platforms, you can continue running the end-to-end tests without rebuilding the platforms.

#### Build Tests Output

The build tests will output the results to the `tests/integration/build-test-results/` directory. Each platform will 
produce a table with the total uncompressed and gzip sizes of the built SDKs.

For example:

|  | Total Uncompressed Size | Total Gzip Size |
| --- | --- | --- |
| vite-7 - esnext | +105.27 KB | +41.24 KB |
| vite-7 - es2015 | +109.61 KB | +42.47 KB |

#### End-to-End Tests Output

The end-to-end tests will generate a set of golden files in the `tests/integration/tests/__golden__` directory. These 
are used to verify the expected output of the SDKs against a previous run. They can also be inspected manually to see the
the data sent by the SDK to the server. 
See the [Chromium Vite 7 esnext golden file](./tests/integration/tests/__golden__/chromium-vite-7-esnext-session.json) as an example.

### Performance Tests

To run the performance tests, use the following command:

```bash
npm run sdk:test:performance:prepare
npm run sdk:test:performance
```

Same as with the build and end-to-end tests, only run the prepare step if the test app changes.

Performance tests will run a full lighthouse audit on the test app with the SDK loaded, and compare the results against a baseline.  
Main metrics compared are:
- Total Blocking Time: Difference in Total Blocking Time: Sum of all time periods between FCP and Time to Interactive, when task length exceeded 50ms, expressed in milliseconds.
- Main Thread Time: Difference in Main Thread Time: Consider reducing the time spent parsing, compiling and executing JS. You may find delivering smaller JS payloads helps with this.
- Script Evaluation Time: Difference in Script Evaluation Time: Consider reducing the time spent parsing, compiling, and executing JS. You may find delivering smaller JS payloads helps with this.

Performance tests will also run a set of custom performance tests that measure the load time, memory usage, and CPU usage 
of the SDKs using the Chrome dev tools protocol. The tests will simulate the SDK creating a large number of spans and logs,
and measure the impact on the performance of the page.

#### Performance Tests Output

The performance tests will output the results to the `tests/performance/test-results/` directory. Artifacts include:
- Lighthouse performance report in HTML format.
- Markdown report with the performance metrics compared to the baseline.
- CDP performance test results in Markdown format.
- JSON requests generated by the SDK during the CDP performance tests.
- JSON tracing data generated by Chrome dev tools protocol during the performance tests that can be 
manually analyzed using the Chrome tracing viewer for more information.

As an example, the performance tests will output a table with the performance metrics compared to the baseline:

Lighthouse Comparison

|  | Difference | Description |
| --- | --- | --- |
| Total Blocking Time | 0 ms | Difference in Total Blocking Time: Sum of all time periods between FCP and Time to Interactive, when task length exceeded 50ms, expressed in milliseconds. [Learn more about the Total Blocking Time metric](https://developer.chrome.com/docs/lighthouse/performance/lighthouse-total-blocking-time/).  |
| Main Thread Time | -109.73 ms | Difference in Main Thread Time: Consider reducing the time spent parsing, compiling and executing JS. You may find delivering smaller JS payloads helps with this. [Learn how to minimize main-thread work](https://developer.chrome.com/docs/lighthouse/performance/mainthread-work-breakdown/)  |
| Script Evaluation Time | -68.60 ms | Difference in Script Evaluation Time: Consider reducing the time spent parsing, compiling, and executing JS. You may find delivering smaller JS payloads helps with this. [Learn how to reduce Javascript execution time](https://developer.chrome.com/docs/lighthouse/performance/bootup-time/).  |

CDP Performance Tests Comparison

|  | Number of Requests | Size of Requests | Script Duration | Task Duration | Heap Used Size |
| --- | --- | --- | --- | --- | --- |
| Requests | +5  requests | +76.51 KB |  |  |  |
| Page Loaded |  |  | +9.43 ms | -42.29 ms | +2.93 MB |
| Generate 100 fetch requests |  |  | +2.70 ms | -4.82 ms | +3.42 MB |
| Generate 100 XHR requests |  |  | +30.66 ms | +75.12 ms | +3.81 MB |
| Click 100 buttons and generate 100 logs |  |  | +181.19 ms | +189.49 ms | +4.50 MB |
| Throw a 100 exceptions |  |  | +51.32 ms | +135.87 ms | +2.57 MB |
| End Session |  |  | +1.29 ms | +5.11 ms | +2.65 MB |
| Total | +5  requests | +76.51 KB | +276.59 ms | +358.48 ms | +19.87 MB |

